% chktex-file 18 disable warnings about "
% chktex-file 19 disable warnings about ô
% chktex-file 6 disable italic correction warnings

\chapter{Histórico}

\section{Inteligência Artificial}

Definir o que é inteligência não é uma tarefa fácil, e inteligência artificial
é mais difícil ainda. Historicamente, existiram duas abordagens para a
definição de inteligência: O comparativo com humanos e o comparativo com seres
racionais.

\subsection{Inteligência Humana}

No primeiro modelo, o seres da espécie humana são tidos como parâmetros e
portanto, qualquer avaliação de inteligência tem que se refletir em
comportamentos ou pensamentos similares aos humanos. O Teste de Turing proposto
pelo matemático Alan Turing (1950) foi uma maneira operacional de estabelecer
inteligência pelo comparativo comportamental. Em sua versão mais simplificada,
um computador poderia ser considerado inteligente se, ao ser interrogado por um
humano com algumas questões, este não pudesse determinar se as respostas eram
oriundas de um computador ou de uma pessoa. A Ciência Cognitiva, por outro
lado, busca compreender o mecanismo pelos quais os humanos são capazes de
pensar e à partir disso estabelecer comparações com algum algoritmo que
eventualmente pudesse replicar estas capacidades.

\subsection{Inteligência Racional}

Além do paradigma humano, existe um outro que é a comparação com seres
racionais. Um ser é dito racional se ele age da ``maneira correta'', dado o que
ele conhece. Isso nos leva a seguinte definição: um ser é inteligente se ele
age ou pensa como um ser racional.

Aristóteles foi o primeiro a sistematizar uma maneira de ``pensar
corretamente''.  Ele foi o criador da disciplina que hoje conhecemos por Lógica
que estabelecia estruturas argumentativas tais como: ``Sócrates é um homem;
todos os homens são mortais; portanto, Sócrates é mortal'' as quais sempre
produzem conclusões corretas. Porém, estes silogismas nem sempre são capazes de
traduzir conhecimento informal, principalmente em face à incertezas e por isso
tendem a não ser reconhecidos como tudo que existe em termos de inteligência.

Além disso existem diferenças entre resolver um problema em princípio e
resolvê-lo na prática. Embora a abordagem pelas ``leis do pensamento'' do
Aristóteles foquem em realizar inferências corretas, esta não é capaz de
exaurir o que significa agir como um ser racional pois existem situações em que
não há uma ação comprovadamente correta, mas em que uma ação deve ser tomada
mesmo assim. Além disso, existem comportamentos racionais que não podem ser
deduzidos por mera inferência como, por exemplo, o reflexo de remover
rapidamente a mão do fogo sem uma cuidadosa pré deliberação sobre o assunto.

O propósito deste trabalho não é exaurir todos os ângulos pelos quais esta
questão já foi abordada, tais como psicologia, lógica e filosofia mas sim
apresentar apenas uma introdução que permita entender o surgimento do o campo da
Inteligência Artificial dentro da ciência da computação.

\section{Histórico da Inteligência Artificial}

\subsection{Primeiros anos}

O primeiro trabalho a ser considerado efetivamente ``Inteligência Artificial''
foi feito em 1943 por Warren McCulloch e Walter Pits, no qual foi proposto o
modelo de funcionamento de um neurônio artificial.  Este neurônio podia assumir
os estados ``ligado'' e ``desligado'', determinado em resposta aos estímulos
proporcionados pelos outros neurônios vizinhos. Foi demonstrado também que uma
rede de neurônios artificial conseguia computar qualquer função computável e
sugerido que a mesma poderia aprender.

Em 1949 Donald Hebb demonstrou uma simples regra pela qual a força ou os pesos
numéricos de conexão entre os neurônios poderia ser modificada, a qual ficou
conhecida como aprendizado hebbiano. Logo em seguida, em 1950, foi construído o
primeiro computador de rede neural chamado de SNARC que fora feito com 3000
tubos de vácuo e conseguia simular uma rede de 40 neurônios.

Embora o neurônio artificial seja considerado o primeiro trabalho, o termo
Inteligência Artificial só viria a ser cunhado mais de 10 anos depois em
Darthmouth nos Estados Unidos em um workshop organizado por John McCarthy que
duraria dois meses e cujo propósito era estudar teoria dos autômatos, redes
neurais e inteligência. Embora não tenha produzido nenhum progresso, o workshop
serviu para introduzir as principais figuras da área entre si.

Nos seus primeiros anos a IA obteve bastante sucesso. Um a um, os items da
lista ``uma máquina nunca vai poder fazer X'' compilada por Turing iam sendo
desbancados. Newel and Simon criaram o primeiro GPS (General Problem Solver),
um programa desenhado para imitar a maneira como humanos pensam ao invés de
simplesmente seguir regras lógicas. Na classe limitada de desafios que
conseguia resolver, a ordem dos subobjetivos que o programa tentava seguir era
similar à humana. Na IBM, Nathaniel Rochester criaria o primeiro Provador de
Teoremas Geométricos, o qual conseguia provar teoremas que vários estudantes
consideravam complexos.  Arthur Samuel refutou a ideia de que programas somente
conseguiriam fazer o que lhes eram dito criando um programa de computador que
conseguia jogar um jogo melhor que o seu criador.

Foi nessa época também que McCarthy criaria sua linguagem de programação
\emph{Lisp} que viraria a linguagem dominante em IA e com a qual ele faria o
primeiro algoritmo que aceitava novos axiomas, sendo então o primeiro autômato
que conseguia adquirir novas capacidades sem necessitar ser reprogramado.
Outras contribuições significativas foram o programa SAINT (1963) que conseguia
resolver problemas de Cálculo típicos do primeiro ano de curso, o programa
ANALOGY (1968) respondia a perguntas de analogia geométrica como as de testes
de QI, o programa STUDENT (1967) resolvia problemas algébricos e o programa
SHRDLU (1972) era capaz de executar instruções do tipo ``Encontre um bloco
maior do que você está segurando agora e o coloque na caixa''. Foi nessa época
também que foram feitas melhorias no processo de aprendizado Hebbiano que
ficaram conhecidas como redes \emph{adalines}.

\subsection{Maturação e Estado da Arte}

O sucesso inicial foi seguido de um período de relativa estagnação. O fato de
que estes algoritmos só conseguiam fazer manipulação simbólica e não conheciam
nada sobre o domínio que trabalhavam provou ser um grande empecilho. Além disso
a maioria dos primeiros programas de IA resolviam o problema tentando várias
combinações até que o resultado fosse atingido, o que não era factível para
instâncias de problemas marginalmente maiores. Foi nesta época que Minsky e
Papert provaram que um neurônio artificial agora conhecido como
\emph{perceptron} era restrito no tipo de funções que conseguia representar e o
financiamento de pesquisas na area se tornou escasso. Já na década de 1970,
houve o surgimento dos sistemas baseados em conhecimento prévio como o programa
DENDRAL que viu uma melhora significativa de desempenho na tarefa de inferir a
estrutura molecular à partir da massa dos seus componentes. O programa que
inicialmente tentava todas as alternativas possíveis passou a utilizar de
conhecimento de especialistas em química para identificar subestruturas
conhecidas o que reduzia significadamente o número de tentativas.

O primeiro caso de sucesso comercial reportado destes sistemas especialistas
era um programa que ajudava na compra de novos computadores e que teria
economizado à empresa US\$40 milhões por ano, o que explica o fato destes
sistemas continuarem sendo usados até hoje.

Paralelamente, houve o ressurgimento das redes neurais no final da década de
1980, proporcionado pela reinvenção do algoritmo de \emph{back-propagation},
uma maneira eficiente de atualizar os pesos das conexões nos neurônios, e
também uma melhoria nas práticas de pesquisa na área que passou a exigir um
maior rigor científico acompanhado do uso de bancos de dados padrões que
permitiam o teste e comparação dos resultados entre diferentes algoritmos. Em
uma outra linha recente, a ênfase no algoritmo da lugar ao uso de grandes
quantidades de dados inspirado no trabalho de Yarowsky (1995) que demostrou ser
possível reconhecer se o uso da palavra \emph{plant} significava uma flor ou uma
fábrica com precisão acima de 96\% sem utilizar de rótulos fornecidos por
humanos, apenas com as definições do dicionário desta palavra. Trabalhos como o
dele sugerem um ``gargalo de conhecimento'' na IA, ou seja, o conhecimento que
o um sistema precisa pra resolver um problema pode vir de grandes massas de
dados ao invés de intervenções de especialistas na área. Hoje em dia, a IA está
em várias aplicações de campos diferentes, como por exemplo:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Na área de robótica veicular com carros auto guiados.
\item
  Na área de reconhecimento de fala.
\item
  Planejamento automático de tarefas.
\item
  Jogos, com destaque para o computador DEEP BLUE da IBM que derrotou o
  campeão mundial de xadrez, Garry Kasparov.
\item
  Controle de SPAN em mensagens de e-mail.
\item
  Militar e Logística.
\end{itemize}

\section{Redes Neurais}

As primeiras definições de uma rede neural artificial foram feitas em 1943 por
McCulloch e Pits inspiradas na hipótese de que as atividades mentais do cérebro
humano consistem em reações eletroquímicas em redes de células cerebrais
chamadas de neurônios. O neurônio artificial consiste em uma unidade
computacional abstrata capaz de receber estímulos de entrada e produzir uma ou
mais saídas e uma rede neural artificial --- de agora em diante referenciada
apenas por \emph{rede neural} --- consiste em um grafo direcionado cujos nodos
são neurônios artificiais.

\begin{figure}
  \caption{Abstração de um neurônio artificial}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsection{O neurônio artificial}

Cada neurônio recebe $m$ entradas ponderadas por um peso $w_m$,
correspondente a entrada do seu i-ésimo vizinho. Esta configuração foi
inspirada nos neurônios naturais cujas interconexões são reguladas por um
potencial de ativação responsável por diferenciar a intensidade entre as
mesmas. A saída é produzida aplicando-se a função limite $\sigma$ na soma
de todos as entradas ponderadas do neurônio. Esta função simula a \emph{lógica
  de limite} existente nos neurônios naturais, os quais somente irão propagar o
seu sinal caso haja um acúmulo suficiente de potencial elétrico no neurônio. Em
sua forma matemática, a saída $y$ do neurônio artificial pode ser expressada da
seguinte forma:

$$y=\sigma(\sum_{i=1}^{m}x_i w_i + b)$$

A função $\sigma$ representa na verdade uma classe de funções que podem ser
utilizadas dependendo do objetivo da construção da rede neural. Para problemas
de classificação \emph{linearmente separáveis}, pode ser usada a função limite
exemplificada na figura~\ref{fig:lim-functin}, enquanto para problemas não
linearmente separáveis é necessário utilizar algum tipo de função
\emph{sigmoide}. A função logística é um exemplo de função sigmoide
frequentemente utilizada por ser diferenciável em todo o seu domínio, o que
permite produzir garantias de convergência nos algoritmos de aprendizagem.

O valor $b$ corresponde ao \emph{bias} aplicado a função e pode ser omitido
simplesmente inserindo-se artificialmente uma nova entrada $x_0 = 1$.

\begin{figure}
\label{fig:lim-fun}
  \caption{Exemplo de funções de limite $\sigma$ usadas em neurônios artificiais}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsubsection{Bias}

Um neurônio artificial pode ser visto como um aproximador de funções. Mais
adiante<TODO, ref>, será explicado que tipo de função pode ser aproximada e em
qual situação, mas antes cabe aqui explicar o objetivo da entrada de
\emph{bias} ou \emph{viés}. Todo neurônio artificial é dito ter pelo menos uma
entrada extra $x_0 = 1$ e consequentemente, o peso extra associado $w_0$
chamado de bias $b$. O bias permite que a função aproximada pelo neurônio não
necessite obrigatoriamente passar pela origem $(0,0)$, o que reduz o erro médio
da aproximação.

\begin{figure}
  \caption{Exemplo de função aproximada com e sem o vetor de bias}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsubsection{Redes neurais}

Em uma rede neural, os neurônios são organizados em \emph{camadas}. Alguns
autores não consideram a camada de entrada como uma camada distinta, mas esta
abstração permite simplificar a notação e torna o processo de conectar redes
neurais entre si mais intuitivo. Outra camada especial é a de saída, pois é
ela que determina se a rede exercerá a função de um \emph{classificador} ou de
um \emph{regressor}. Todas as demais camadas intermediárias são chamadas de
\emph{camadas ocultas}; o nome não possui nenhuma conotação especial e serve
apenas para diferenciá-las das camadas de entrada e saída. Em uma rede
\emph{feed forward} as saídas de cada camada somente são computadas na direção
e sentido da camada de saída, isto é, não há uma retroalimentação das conexões
de saída para as de entrada. Quando isto acontece a rede é chamada de
\emph{recorrente} porém, o estudo deste tipo de rede neural não esta nos
objetivos deste trabalho.

Sendo assim, uma rede neural necessita ter pelo menos duas camadas --- a de
entrada e a de saída. A adição de camadas ou neurônios não possui um efeito
óbvio e generalizável para qualquer rede, e portanto é alvo de constante
experimentação e observação empírica sujeito a comportamento variável
dependendo do problema.

\begin{figure}
\label{fig:003-nn}
  \caption{Exemplo de uma rede neural de 3 camadas}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

Cada camada $S$ é formada por $R$ neurônios, cujos pesos das conexões podem ser
representados através de uma matriz $W$. A vetorização da computação dos pesos
das camadas permite que operações mais eficientes sejam utilizadas para
realizar a computação da saída da rede neural e do ajuste dos pesos.

$$ W_{S,R} =
\begin{bmatrix}
  w_{1,1} & w_{1,2} & \cdots & w_{1,R} \\
  w_{2,1} & w_{2,2} & \cdots & w_{2,R} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  w_{S,1} & w_{S,2} & \cdots & w_{S,R}
\end{bmatrix}
$$

Com isto o resultado final da rede neural pode ser representado pela equação:

$$ a^{S} = \sigma(Wa^{S-1}+b)$$

O símbolo $a$ representa o vetor de \emph{ativação} dos neurônios da camada S.

\subsubsection{Aprendizagem}

A aprendizagem em uma rede neural se da pelo ajuste dos pesos $w$ das conexões
dos neurônios. Ao longo dos anos foram propostas várias maneiras de atualizar
os pesos de uma rede neural automaticamente.

\begin{figure}
\label{fig:002-limit-functions}
  \caption{Exemplo de funções de ativação}
  \begin{center}
    \includegraphics[height=8cm]{placeholder}
  \end{center}
  \legend{Na esquerda uma simples função de limite; à direita a função $\frac{1}{1+e^{-x}}$}
\end{figure}

Quando a função de ativação $g$ representada na~\ref{fig:001-ann} for uma
função limite[\ref{fig:002-limit-functions}], o neurônio artificial é chamado
de \emph{perceptron}; Quando a função de ativação $g$ for uma função logística,
o neurônio artificial é chamado de \emph{perceptron sigmoide} --- devido ao
fato de que a função logística é um tipo particular de sigmoide. Estas funções
são usadas para garantir que o neurônio consiga reproduzir funções não lineares
sendo a principal vantagem da função sigmoide a sua diferenciabilidade em todo
o seu domínio, fator que mais tarde será relevante na atualização dos pesos das
conexões na rede.

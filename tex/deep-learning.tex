% chktex-file 18 disable warnings about "
% chktex-file 19 disable warnings about ô
% chktex-file 6 disable italic correction warnings

\section{Deep Learning}

\subsection{Por que usar deep learning}

Hoje sabemos\citet{hubel1959receptive} que o processamento visual no cérebro
humano ocorre de maneira gradual e que diferentes regiões são responsáveis pelo
processamento de diferentes \emph{features} extraídas do estímulo sensorial.
Primeiro são detectadas abstrações simples como bordas, que por sua vez são
combinadas em funções mais complexas como detecção de movimento e
reconhecimento facial. Embora esta seja uma boa fonte de inspiração para o uso
de \emph{deep neural networks}, pesquisadores da área hesitam em comparar o que
acontece em uma rede neural artificial com o que acontece dentro de um cérebro
natural, visto que o completo funcionamento do mesmo é um problema científico
em aberto.

A motivação para o uso de uma rede neural \emph{deep} ao invés das arquiteturas
tradicionais \emph{shallow}, tais como \emph{support vector machines} ou redes
neurais com poucas camadas, visto que estas conseguem simular qualquer outra, é
que existe um \emph{tradeoff} entre espaço de memória e tempo de computação que
pode ser explorado para acelerar o processamento das redes.  Uma maneira de
entender este \emph{tradeoff} em redes neurais, é imaginar a saída das camadas
escondidas como resultados intermediários da computação.
\emph{shallow}\cite{bengio2007scaling}

Além disso, até bem pouco tempo atras, os melhores resultados nas tarefas de
classificação e detecção de imagens, videos, sons e linguagem natural
necessitavam de um amplo conhecimento \emph{a priori} por parte dos cientistas
cujo trabalho era elaborar \emph{filtros} capazes de sintetizar toda a
complexidade destas tarefas.

Devido ao alto custo do desenvolvimento e pesquisa destes filtros, surge a
necessidade de um método que consiga aprendê-los de maneira automática, mas que
ainda assim mantenha a precisão dos filtros artesanais. Estes métodos hoje são
conhecidos como \emph{deep learning} e envolvem um gama variada de técnicas
para contornar os problemas de arquiteturas com múltiplas camadas, a fim de
extrair o benefício do tradeoff entre memória e computação mencionado.

\subsection{Problemas com deep learning}

Já que redes neurais já são conhecidas há algum tempo, é de se estranhar que
ninguém tenha tido a ideia de acrescentar mais camadas nestas redes
anteriormente. O fato é que simplesmente adicionar mais camadas incorre em
problemas que até alguns anos atrás eram difíceis de serem contornados.

\begin{description}

  \item[overfitting:] Uma rede com múltiplas camadas tende a apresentar um
    número maior de parâmetros que podem ser treinados, o que leva a um
    problema de \emph{overfitting}, ou seja, a rede simplesmente aprende o
    conjunto de dados de treinamento, mas não é muito capaz de realizar boas
    inferências sobre novos conjuntos de dados. Este problema foi combatido com
    a combinação de algoritmos de aprendizagem não supervisionada que atuam
    como regularizadores durante a fase de pré treinamento.

  \item[saturação do gradiente:] Para poder aprender funções convexas
    complexas, é necessário que os neurônios da rede neural apliquem algum tipo
    de não linearidade em seu processamento. Porém, o algoritmo de
    \emph{backpropagation}, utilizado na aprendizagem, pode sofrer do fenômeno
    de saturação do gradiente. Isto ocorre em neurônios cujo valor de ativação
    estejam próximos dos extremos da função de ativação fazendo com que o
    gradiente fique próximo de zero. Esta saturação também propagada para as
    camadas mais inferiores da rede.

  \item[processamento intensivo:] Mesmo com estas técnicas, hoje em dia o
    processamento de redes profundas é predominantemente feito em GPUs, visto
    que estes hardwares são altamente especializados em processamento vetorial
    paralelo. O crescente suporte dos fabricantes destes hardwares e de
    frameworks e bibliotecas que facilitam a utilização dos mesmos também foi
    importante fator para tornar o uso de redes com múltiplas camadas viável.

\end{description}

\subsection{Aprendizado não supervisionado}

Aprendizado não supervisionado é usado para regularizar uma rede neural a fim
de impedir que a mesma apresente um alto grau de over fitting. Os algoritmos
mais empregados para esta tarefa são \emph{autoencoders} e \emph{Restricted
  Boltzman Machines}, que atuam no pré treinamento de redes neurais profundas
como regularizadores, fazendo uma pré calibragem dos parâmetros $W$ antes da
rede ser completamente conectada e \emph{fine tuned}. Este processo limita os
valores dos pesos das conexões da rede para um intervalo relativamente pequeno
e tem gerado bons resultados. Esta combinação de métodos supervisionados com
não supervisionados é conhecido como aprendizado semi supervisionado.

\subsubsection{Autoencoders}

Autoencoders são redes neurais de representação binária com uma única camada,
cuja \emph{loss function} usa a própria entrada como alvo de aprendizagem. Caso
o número de neurônios na camada intermediária seja menor que o número na camada
de entrada, então o autoencoder aprende uma representação mais compacta para os
dados de entrada, de fato, dependendo da função de erro usada, um autoencoder
realiza fatoração de componentes principais na entrada. Se o número de
neurônios na camada intermediária for maior ou igual ao número de neurônios na
entrada então os autoencoder não tem grande utilidade, exceto nos
\emph{denoising autoencoders} onde cada neurônio da entrada original é
corrompido (setado para 0) com uma probabilidade $p$. Isto confere ao
autoencoder um pouco de invariabilidade a ruído na rede, o que melhora a
generalização.

\begin{figure}
  \caption{Exemplo de Autoencoder}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsubsection{Restricted Boltzman Machines}

\emph{Restricted Boltzman Machine} é um exemplo de um formalismo conhecido como
Modelo Baseado em Energia (EBM). O objetivo de um EBM é encontrar uma função de
energia $E(X,Y)$ que associa um valor de entrada $X$ e um valor de saída $Y$ a
um escalar chamado de energia.  Uma boa função de energia é aquela que associa
um valor baixo para uma configuração $(X,Y)$ desejada, e um valor de energia
alto caso contrário.\citep{lecun2006tutorial}

Porém, uma Restricted Boltzman Machine é não supervisionada, portanto, a
variável $Y$ dos rótulos conhecidos é substituída por uma variável escondida
$h$. A representação mais comum de uma RBM é um grafo bipartido não direcionado
completamente conexo, onde uma das partições do grafo representa a entrada que
é visível $v$, e a outra representa a variáveis escondidas do modelo $h$.
Depois de um aprendizado realizado com sucesso, uma RBM contém a distribuição
probabilísticas das observações de treinamento.  Essa distribuição é expressa
pela probabilidade conjunta destas duas variáveis:

$$ p(v,h) = \frac{\exp(-E(v,h))}{Z}$$

Onde, $E(v,h)$ é a \emph{função de energia} que é dada pela fórmula:

$$E(v,h)=\mathbf{-bv^T-ch^T-h^{T}Wv}$$

Onde $\mathbf{b}$ e $\mathbf{c}$ são \emph{bias vectors} e $\mathbf{W}$ é a
matriz de pesos entre a camada visível e a camada escondida. Intuitivamente, o
objetivo destes parâmetros é armazenar uma preferência por uma configuração de
variáveis em particular. Além, disso a função de normalização $Z$ é dada por:

$$ Z = \sum_v \sum_h \exp(-E(v,h)) $$

Esta função geralmente não é tratável.

\begin{figure}
  \caption{Exemplo de RBM}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsection{Convolutional Neural Networks}

\subsubsection{Um breve histórico das CNNs}

Embora as CNNs já fossem conhecidas há bastante tempo, o modelo tradicional
para abordagem de problemas de classificação com \emph{Convolutional Neural
  Networks} envolvia o desenvolvimento de filtros criados de maneira
"artesanal"~por humanos. Na prática, isto limitava muito a aplicação de CNNs e
logo ficou clara a necessidade de um processo que pudesse realizar o
aprendizado destes filtros de maneira automática. A seguir, a listagem adaptada
de~\citet{cs231n}, mostra a evolução destas técnicas principalmente na última
década.

\begin{itemize}

  \item \emph{Neocognitron (1988)}: Criado por
    \citet{fukushima1988neocognitron}, foi o precursor das redes neurais
    convolucionais modernas. Consiste em múltiplas camadas de dois tipos de
    neurônios artificiais que tinham por objetivo simular a configuração
    observada por \citet{hubel1959receptive} em cérebros de gatos, os quais
    possuíam células responsáveis por tarefas distintas no reconhecimento
    visual.

  \item \emph{LeNet (1990)}: Proposta por \citet{le1990handwritten} foi usada
    com sucesso para reconhecimento de caracteres escritos a mão usando o
    dataset~MNIST.\@

  \item \emph{AlexNet (2012)}: \citet{krizhevsky2012imagenet} Foi a ganhadora
    da competição ILSVRC de 2012 e foi a responsável por grande parte do
    entusiasmo atual com redes convolucionais. Utiliza técnicas como
    \emph{droppout} e \emph{ReLUs}.

  \item \emph{ZF Net (2013)}: Se destacou na competição ILSVRC de 2013, por
    apresentar melhorias em relação à AlexNet, com a otimização de
    hyperparametros e a expansão do número de layers intermediários.
    \citet{zeiler2014visualizing}

  \item \emph{GoogLeNet (2014)}: A principal contribuição de
    \citet{szegedy2014going} foi o desenvolvimento de um \emph{Inception
      Module} que conseguiu reduzir o número de parâmetros (pesos) em 15 vezes.

  \item \emph{VGGNet (2014)}: \citet{zeiler2014visualizing} trouxe sua
    contribuição demonstrando que a profundidade da arquitetura é um componente
    crucial do desempenho da rede. Mais tarde foi descoberto que a sua
    performance supera a da GoogLeNet, embora necessite de mais memória e
    parâmetros. Hoje, é a rede preferida para extrair \emph{features} de
    imagens.

\end{itemize}

\subsubsection{VGGNet}

Até o presente momento, o modelo de \citep{zeiler2014visualizing} --- conhecido
como VGGNet --- é tido como um dos melhores, obtendo precisão semelhante à
humanos \citep{human}, com uma taxa de erro na tarefa de classificação da ILSVRC
de 6,8\% quando a classe correta da imagem não está entre as cinco primeiras
sugeridas pelo modelo. A competição deste ano (2015) ainda não teve seu
resultado anunciado, mas é provável que hajam novas melhorias nos modelos
atuais. Mesmo assim, a VGGNet ainda é um caso relevante para estudo.

\subsubsection{Arquitetura e Modelo}

A arquitetura usada pode ser representada da seguinte maneira:~\cite{cs231n}

\begin{footnotesize}
\begin{verbatim}
INPUT:     [224x224x3]   memory: 224*224*3=150K   weights: 0
CONV3-64:  [224x224x64]  memory: 224*224*64=3.2M  weights: (3*3*3)*64 = 1,728
CONV3-64:  [224x224x64]  memory: 224*224*64=3.2M  weights: (3*3*64)*64 = 36,864
POOL2:     [112x112x64]  memory: 112*112*64=800K  weights: 0
CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728
CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456
POOL2:     [56x56x128]   memory: 56*56*128=400K   weights: 0
CONV3-256: [56x56x256]   memory: 56*56*256=800K   weights: (3*3*128)*256 = 294,912
CONV3-256: [56x56x256]   memory: 56*56*256=800K   weights: (3*3*256)*256 = 589,824
CONV3-256: [56x56x256]   memory: 56*56*256=800K   weights: (3*3*256)*256 = 589,824
POOL2:     [28x28x256]   memory: 28*28*256=200K   weights: 0
CONV3-512: [28x28x512]   memory: 28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
CONV3-512: [28x28x512]   memory: 28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [28x28x512]   memory: 28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
POOL2:     [14x14x512]   memory: 14*14*512=100K   weights: 0
CONV3-512: [14x14x512]   memory: 14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]   memory: 14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]   memory: 14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
POOL2:     [7x7x512]     memory: 7*7*512=25K      weights: 0
FC:        [1x1x4096]    memory: 4096             weights: 7*7*512*4096 = 102,760,448
FC:        [1x1x4096]    memory: 4096             weights: 4096*4096 = 16,777,216
FC:        [1x1x1000]    memory: 1000             weights: 4096*1000 = 4,096,000

TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
TOTAL params: 138M parameters

\end{verbatim}
\end{footnotesize}

\emph{INPUT}, \emph{CONV}, \emph{POOL} e \emph{FC} representam camadas na rede
com funções específicas. INPUT, CONV e POOL são auto explicativos e FC
significa \emph{Fully Connected}. Os números ao lado do nome da camada se
referem a características dos filtros que devem ser aprendidos. CONV3\-64, por
exemplo, indica a presença de 64 filtros cuja distância espacial (altura e
largura) é igual a 3. Na camada de pooling, somente é representado a distância
espacial, ou seja 2.

As considerações de memória são cruciais visto que redes convolucionais
modernas dependem de placas gráficas para acelerar o seu treinamento e a
maioria destas possuem limites de memória que vão de 3 até 6GB\@.


O grande diferencial da VGGNet para a sua maior competidora, a GoogLeNet, é o
uso de filtros pequenos \emph{$F=3$}, os quais aceleram o treinamento da rede.
Além disso a rede apresenta uma profundidade (número de camadas) bastante
elevado (até 19), corroborando a ideia de que a profundidade da rede é
fundamental para um bom desempenho.

\subsection{Funcionamento}

Redes convolucionais são um tipo especial de \emph{deep networks} projetadas
especificamente para lidar com dados vetoriais tais como imagens e sons.  Este
tipo de rede neural ganhou fama na competição ILSVRC realizada em 2012 na qual
conseguiu-se diminuir pela metade o erro do melhor competidor. Este feito foi
atingido com o uso de ReLUs (\emph{rectified linear units}), GPUs e uma técnica
conhecida como \emph{dropout}. Desde então, Redes Convolucionais ganharam força
e hoje são as melhores técnicas para reconhecimento de
imagens.\cite{lecun2015deep}

Uma rede convolucional é baseada em 4 ideias principais: conexões locais, pesos
compartilhados, \emph{pooling} e múltiplas camadas. Conexões locais exploram o
fato de que em uma imagem existe uma correlação entre os valores dos pixels
vizinhos. Já pesos compartilhados, geram o conceito de \emph{feature maps} e
reduzem o número de pesos que precisam ser aprendidos e a operação de
\emph{pooling} (geralmente o máximo local) é usada pois é invariante a posição,
o que da um maior poder de abstração para a rede.\cite{lecun2015deep}

A operação de filtro é realizada pela função matemática de convolução, cujo
papel é detectar conjunções locais nas \emph{features} da camada anterior. Logo
em seguida, vem a camada de \emph{pooling} cujo papel é combinar
\emph{features} semanticamente semelhantes.\cite{lecun2015deep}

A rede é tipicamente composta de dois a três estágios de convolução seguidos da
aplicação de não linearidade e de \emph{pooling}. Por fim, são aplicadas mais
camadas convolucionais, desta vez, completamente conectadas. O aprendizado é
feito através de \emph{backpropagation} como em redes neurais
tradicionais.\cite{lecun2015deep}

\subsubsection{Hyper parâmetros}

O termo "hiper parâmetros" se refere as "alavancas" que um \emph{designer} pode
puxar enquanto decide a arquitetura da rede convolucional. Além dos hyper
parâmetros tradicionalmente conhecidos das redes neurais convencionais, tais
como, quantidade de neurônios por camada, função de não linearidade, taxa de
aprendizado e função de regularização, as redes convolucionais contém alguns
parâmetros que são exclusivos para o seu funcionamento:

\begin{itemize}

  \item \emph{Número de filtros ($K$):} Este parâmetro regula quantos filtros
    serão treinados na rede.
  \item \emph{Stride ($S$):} Se refere ao passo que é dado pela
    janela de convolução na imagem original.
  \item \emph{Distância espacial ($F$):}
    Se refere a largura e altura dos filtros.
  \item \emph{Padding ou Preenchimento
      ($P$):} Se refere a largura e altura dos filtros.

\end{itemize}

Porém nem toda combinação destes parâmetros é válida devido a propriedades
geométricas. Considerando um volume de entrada de dimensões: $W_1 \times H_1
\times D_1$, produzirá um volume de tamanho: $W_2 \times H_2 \times D_2$ sendo
que: $$ W_2= \frac{(W_1-F+2P)}{S+1} $$ $$ H_2= \frac{(H_1-F+2P)}{S+1} $$ $$
D_2= F $$ Uma combinação válida destes parâmetros produz um valor de $W_2$ (ou
$H_2$) inteiro.

Assumindo o uso de \emph{compartilhamento de parâmetros}, cada filtro adiciona
um total de $(F \cdot F \cdot D_1)\cdot K$ pesos e $K$ \emph{biases}.

A camada de pooling também possui seu próprio parâmetro $F$ e $S$ e seu
respectivo volume de saída terá as dimensões:

$$W_2 = (W_1 - F)/S + 1$$
$$H_2 = (H_1 - F)/S + 1$$
$$D_2 = D_1$$
\begin{figure}
  \caption{Exemplo de configuração espacial}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}
\subsubsection{Dropout}

O objetivo da técnica de \emph{dropout} é aproximar a solução ótima de calcular
a média dos resultados de treinamento de todas as combinações de parâmetros
possíveis. Isto é feito pela remoção aleatória e temporária de unidades da rede
bem como suas conexões. Cada unidade é mantida na rede com uma probabilidade
$p$, independente das outras.\cite{srivastava2014dropout}

Com a aplicação de \emph{dropout} há uma redução no \emph{overfitting} e uma
melhoria na regularização, porém, o treinamento da rede tende a demorar mais
gerando um \emph{tradeoff} entre \emph{overfitting} e tempo de
treinamento.\cite{srivastava2014dropout}

Está técnica não está restrita somente a redes \emph{feed forward} e pode ser
aplicadas a \emph{RBM} e \emph{autoencoders}, proporcionando uma maneira de
combinar exponencialmente diferentes arquiteturas
eficientemente.\cite{srivastava2014dropout}

\begin{figure}
  \caption{Uma rede neural com a aplicação de dropout}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsubsection{ReLU}

Em uma rede neural tradicional, as funções $\tanh(x)$ e $\frac{1}{1+e^{-x}}$
são usadas para introduzir não linearidade na computação do produto dos pesos
pelas entradas dos neurônios. Porém, estas funções possuem a desvantagem de
apresentarem o fenômeno de saturação. Isto ocorre quando a entrada da função
está distante da ponto central ($x=0$) e provoca uma perda de desempenho no
aprendizado.\cite{krizhevsky2012imagenet}

Por causa disto, o uso de \emph{rectifiers} tem se tornado cada vez mais comum
no treinamento de grandes redes. Este \emph{rectifier} é definido como:

$$ f(x) = \max(0,x) $$

Quando uma unidade ou neurônio usa este \emph{rectifier} ele é chamado de
Rectified Linear Unit ou ReLU.\@ De acordo com \citep{krizhevsky2012imagenet},
ReLUs são muito mais rápidos do que \emph{saturating non linearities} --- como
são conhecidas as funções mencionadas anteriormente --- e não requerem
regularização, embora isto ainda seja usado para aumentar o poder de
generalização da rede.

\begin{figure}
  \caption{Tipos de não linearidades aplicadas}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsubsection{Regularização}

Em uma típica rede \emph{deep}, existem milhares de pesos que sofrem adaptação
em seus valores. Normalmente um aproximador de função com este grande número de
parâmetros ajustáveis está sujeito a \emph{over fitting}. Isto reduz a
capacidade de generalização do modelo gerado para entradas que não fizeram
parte do treinamento. Redes neurais contornam este problema com o uso de
técnicas de regularização tais como:

\begin{itemize}

  \item \emph{Regularização L2}: Para cada peso $w$ da rede, é
    adicionado uma penalidade $\frac{1}{2}\lambda w^2$, onde $\lambda$ é a
    força de regularização. O termo $\frac{1}{2}$ é adicionado para facilitar o
    cálculo da derivada na fase de \emph{backpropagation} e o termo todo tem a
    interpretação de penalizar vetores de pesos com alguns poucos pesos muito
    altos e incentivar o uso de vetores difusos, ou seja, eles incentivam a
    rede a utilizar todos as suas entradas ao invés de usar demasiadamente uma
    entrada ou outra.

  \item \emph{Regularização L1}: Para cada peso $w$ da rede, é adicionado uma
    penalidade $\lambda|w|$ com a finalidade de tornar os pesos na rede
    esparsos (próximos de zero). Isto confere a rede um certo grau de
    invariabilidade à entradas com ruído. A regularização L1 pode ser usada
    juntamente com a L2, sendo chamadas neste caso de \emph{Elastic net
      regularization}.

  \item \emph{Max norm constraints}: Consiste em estipular um valor máximo para
    os pesos $w$ de maneira que a rede não se torne instável mesmo na presença
    de uma taxa de aprendizado muito grande.

  \item \emph{Dropout}: A técnica de \emph{dropout} também pode ser vista como
    uma regularização e pode ser usada juntamente com outros tipos.

\end{itemize}

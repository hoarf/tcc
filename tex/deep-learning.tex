% chktex-file 18 disable warnings about "
% chktex-file 19 disable warnings about ô
% chktex-file 6 disable italic correction warnings

\section{Deep Learning}

\subsection{Introdução}

A área de \emph{deep structured learning} que vem sendo conhecida como
\emph{deep learning} surgiu como um novo campo de pesquisa em aprendizado de
máquina nos últimos anos e as técnicas desenvolvidas nesta área já tem
impactado uma gama variada de trabalhos em processamento de sinais e
informações. Embora hajam diferentes definições, este trabalho adotará a
definição contida em \url{https://github.com/lisa-lab/DeepLearningTutorials}:
\cite{deng2014deep}

\newtheorem{def-deep-learning}{Definição}

\begin{def-deep-learning}

  \begin{quote}

    \emph{Deep Learning} é uma nova área do aprendizado de máquina que vem
    sendo introduzida com o objetivo de aproximar o Aprendizado de Máquina do
    seu objetivo original: Inteligência Artificial. \emph{Deep Learning} aborda
    múltiplos níveis de representação e abstração que ajudam a processar dados
    tais como sons, imagens e texto.

  \end{quote}

\end{def-deep-learning}

A motivação para o uso de uma rede neural \emph{deep} ao invés das arquiteturas
tradicionais \emph{shallow}, tais como \emph{Support Vector Machines}, visto que
esta última, a princípio consegue simular qualquer outra, é que existe um
\emph{tradeoff} entre espaço de memória e tempo de computação que pode ser
explorado para acelerar o processamento das redes. Uma maneira de entender este
\emph{tradeoff} em redes neurais é imaginar a saída das camadas intermediárias
como resultados intermediários da computação de uma rede.
\emph{shallow}\cite{bengio2007scaling}

Ainda assim,\emph{deep supervised learning} é uma tarefa difícil devido ao uso
de \emph{loss functions} não convexas presentes em problemas de grande
importância como computação a visual que apresentam a tendência de ficarem
presas em mínimos locais. Foi somente com os recentes avanços nas técnicas de
treinamento e arquiteturas de placas de vídeos que hoje é possível treinar
redes com milhares de pesos em um tempo hábil. Nas próximas sessões, será
apresentado o histórico e funcionamento destas redes e procedimentos.

\subsection{Taxonomia de redes neurais deep}

O campo das redes neurais de múltiplas camadas possui técnicas que são
altamente combinadas e recombinadas entre si, o que gera uma explosão de termos
e conceitos. A sintese a seguir abrange os principais conceitos que permeiam
o campo das redes neurais profundas.

\begin{description}

  \item[autoencoders:] Um autoassociador ou \emph{autoencoder} é uma rede neural onde a própria camada de entrada é usada como alvo da camada de saída. \ 

\end{description}

\subsection{Convolutional Neural Networks}

\subsubsection{Um breve histórico das CNNs}

Embora as CNNs já fossem conhecidas há bastante tempo, o modelo tradicional
para abordagem de problemas de classificação com \emph{Convolutional Neural
  Networks} envolvia o desenvolvimento de filtros criados de maneira
"artesanal"~por humanos. Na prática, isto limitava muito a aplicação de CNNs e
logo ficou clara a necessidade de um processo que pudesse realizar o
aprendizado destes filtros de maneira automática. A seguir, a listagem adaptada
de~\citet{cs231n}, mostra a evolução destas técnicas principalmente na última
década.

\begin{itemize}

  \item \emph{Neocognitron (1988)}: Criado por
    \citet{fukushima1988neocognitron}, foi o precursor das redes neurais
    convolucionais modernas. Consiste em múltiplas camadas de dois tipos de
    neurônios artificiais que tinham por objetivo simular a configuração
    observada por \citet{hubel1959receptive} em cérebros de gatos, os quais
    possuíam células responsáveis por tarefas distintas no reconhecimento
    visual.

  \item \emph{LeNet (1990)}: Proposta por \citet{le1990handwritten} foi usada
    com sucesso para reconhecimento de caracteres escritos a mão usando o
    dataset~MNIST.\@

  \item \emph{AlexNet (2012)}: \citet{krizhevsky2012imagenet} Foi a ganhadora
    da competição ILSVRC de 2012 e foi a responsável por grande parte do
    entusiasmo atual com redes convolucionais. Utiliza técnicas como
    \emph{droppout} e \emph{ReLUs}.

  \item \emph{ZF Net (2013)}: Se destacou na competição ILSVRC de 2013, por
    apresentar melhorias em relação à AlexNet, com a otimização de
    hyperparametros e a expansão do número de layers intermediários.
    \citet{zeiler2014visualizing}

  \item \emph{GoogLeNet (2014)}: A principal contribuição de
    \citet{szegedy2014going} foi o desenvolvimento de um \emph{Inception
      Module} que conseguiu reduzir o número de parâmetros (pesos) em 15 vezes.

  \item \emph{VGGNet (2014)}: \citet{zeiler2014visualizing} trouxe sua
    contribuição demonstrando que a profundidade da arquitetura é um componente
    crucial do desempenho da rede. Mais tarde foi descoberto que a sua
    performance supera a da GoogLeNet, embora necessite de mais memória e
    parâmetros. Hoje, é a rede preferida para extrair \emph{features} de
    imagens.

\end{itemize}

\subsubsection{VGGNet}

Até o presente momento, o modelo de \citep{zeiler2014visualizing} --- conhecido
como VGGNet --- é tido como um dos melhores, obtendo precisão semelhante à
humanos \citep{human}, com uma taxa de erro na tarefa de classificação da ILSVRC
de 6,8\% quando a classe correta da imagem não está entre as cinco primeiras
sugeridas pelo modelo. A competição deste ano (2015) ainda não teve seu
resultado anunciado, mas é provável que hajam novas melhorias nos modelos
atuais. Mesmo assim, a VGGNet ainda é um caso relevante para estudo.

\subsubsection{Arquitetura e Modelo}

A arquitetura usada pode ser representada da seguinte maneira:~\cite{cs231n}

\begin{footnotesize}
\begin{verbatim}
INPUT:     [224x224x3]   memory: 224*224*3=150K   weights: 0
CONV3-64:  [224x224x64]  memory: 224*224*64=3.2M  weights: (3*3*3)*64 = 1,728
CONV3-64:  [224x224x64]  memory: 224*224*64=3.2M  weights: (3*3*64)*64 = 36,864
POOL2:     [112x112x64]  memory: 112*112*64=800K  weights: 0
CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728
CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456
POOL2:     [56x56x128]   memory: 56*56*128=400K   weights: 0
CONV3-256: [56x56x256]   memory: 56*56*256=800K   weights: (3*3*128)*256 = 294,912
CONV3-256: [56x56x256]   memory: 56*56*256=800K   weights: (3*3*256)*256 = 589,824
CONV3-256: [56x56x256]   memory: 56*56*256=800K   weights: (3*3*256)*256 = 589,824
POOL2:     [28x28x256]   memory: 28*28*256=200K   weights: 0
CONV3-512: [28x28x512]   memory: 28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
CONV3-512: [28x28x512]   memory: 28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [28x28x512]   memory: 28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
POOL2:     [14x14x512]   memory: 14*14*512=100K   weights: 0
CONV3-512: [14x14x512]   memory: 14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]   memory: 14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]   memory: 14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
POOL2:     [7x7x512]     memory: 7*7*512=25K      weights: 0
FC:        [1x1x4096]    memory: 4096             weights: 7*7*512*4096 = 102,760,448
FC:        [1x1x4096]    memory: 4096             weights: 4096*4096 = 16,777,216
FC:        [1x1x1000]    memory: 1000             weights: 4096*1000 = 4,096,000

TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
TOTAL params: 138M parameters

\end{verbatim}
\end{footnotesize}

\emph{INPUT}, \emph{CONV}, \emph{POOL} e \emph{FC} representam camadas na rede
com funções específicas. INPUT, CONV e POOL são auto explicativos e FC
significa \emph{Fully Connected}. Os números ao lado do nome da camada se
referem a características dos filtros que devem ser aprendidos. CONV3\-64, por
exemplo, indica a presença de 64 filtros cuja distância espacial (altura e
largura) é igual a 3. Na camada de pooling, somente é representado a distância
espacial, ou seja 2.

As considerações de memória são cruciais visto que redes convolucionais
modernas dependem de placas gráficas para acelerar o seu treinamento e a
maioria destas possuem limites de memória que vão de 3 até 6GB\@.


O grande diferencial da VGGNet para a sua maior competidora, a GoogLeNet, é o
uso de filtros pequenos \emph{$F=3$}, os quais aceleram o treinamento da rede.
Além disso a rede apresenta uma profundidade (número de camadas) bastante
elevado (até 19), corroborando a ideia de que a profundidade da rede é
fundamental para um bom desempenho.

\subsection{Funcionamento}

Redes convolucionais são um tipo especial de \emph{deep networks} projetadas
especificamente para lidar com dados vetoriais tais como imagens e sons.
Embora já fosse conhecida há algum tempo na área, este tipo de rede neural
ganhou fama na competição ILSVRC realizada em 2012 na qual conseguiu-se
diminuir pela metade o erro do melhor competidor. Este feito foi atingido com o
uso de ReLUs (\emph{rectified linear units}), GPUs e uma técnica conhecida como
\emph{dropout}. Desde então, Redes Convolucionais ganharam força e hoje são as
melhores técnicas para reconhecimento de imagens.\cite{lecun2015deep}

Uma rede convolucional é baseada em 4 ideias principais: conexões locais, pesos
compartilhados, \emph{pooling} e múltiplas camadas. Conexões locais exploram o
fato de que em uma imagem existe uma correlação entre os valores dos pixels
vizinhos. Já pesos compartilhados, geram o conceito de \emph{feature maps} e
reduzem o número de pesos que precisam ser aprendidos e a operação de
\emph{pooling} (geralmente o máximo local) é usada pois é invariante a posição,
o que da um maior poder de abstração para a rede.\cite{lecun2015deep}

A operação de filtro é realizada pela função matemática de convolução, cujo
papel é detectar conjunções locais nas \emph{features} da camada anterior. Logo
em seguida, vem a camada de \emph{pooling} cujo papel é combinar
\emph{features} semanticamente semelhantes.\cite{lecun2015deep}

A rede é tipicamente composta de dois a três estágios de convolução seguidos da
aplicação de não linearidade e de \emph{pooling}. Por fim, são aplicadas mais
camadas convolucionais, desta vez, completamente conectadas. O aprendizado é
feito através de \emph{backpropagation} como em redes neurais
tradicionais.\cite{lecun2015deep}

\subsubsection{Hyper parâmetros}

O termo "hiper parâmetros" se refere as "alavancas" que um \emph{designer} pode
puxar enquanto decide a arquitetura da rede convolucional. Além dos hyper
parâmetros tradicionalmente conhecidos das redes neurais convencionais, tais
como, quantidade de neurônios por camada, função de não linearidade, taxa de
aprendizado e função de regularização, as redes convolucionais contém alguns
parâmetros que são exclusivos para o seu funcionamento:

\begin{itemize}

  \item \emph{Número de filtros ($K$):} Este parâmetro regula quantos filtros
    serão treinados na rede.
  \item \emph{Stride ($S$):} Se refere ao passo que é dado pela
    janela de convolução na imagem original.
  \item \emph{Distância espacial ($F$):}
    Se refere a largura e altura dos filtros.
  \item \emph{Padding ou Preenchimento
      ($P$):} Se refere a largura e altura dos filtros.

\end{itemize}

Porém nem toda combinação destes parâmetros é válida devido a propriedades
geométricas. Considerando um volume de entrada de dimensões: $W_1 \times H_1
\times D_1$, produzirá um volume de tamanho: $W_2 \times H_2 \times D_2$ sendo
que: $$ W_2= \frac{(W_1-F+2P)}{S+1} $$ $$ H_2= \frac{(H_1-F+2P)}{S+1} $$ $$
D_2= F $$ Uma combinação válida destes parâmetros produz um valor de $W_2$ (ou
$H_2$) inteiro.

Assumindo o uso de \emph{compartilhamento de parâmetros}, cada filtro adiciona
um total de $(F \cdot F \cdot D_1)\cdot K$ pesos e $K$ \emph{biases}.

A camada de pooling também possui seu próprio parâmetro $F$ e $S$ e seu
respectivo volume de saída terá as dimensões:

$$W_2 = (W_1 - F)/S + 1$$
$$H_2 = (H_1 - F)/S + 1$$
$$D_2 = D_1$$
\begin{figure}
  \caption{Exemplo de configuração espacial}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}
\subsubsection{Dropout}

O objetivo da técnica de \emph{dropout} é aproximar a solução ótima de calcular
a média dos resultados de treinamento de todas as combinações de parâmetros
possíveis. Isto é feito pela remoção aleatória e temporária de unidades da rede
bem como suas conexões. Cada unidade é mantida na rede com uma probabilidade
$p$, independente das outras.\cite{srivastava2014dropout}

Com a aplicação de \emph{dropout} há uma redução no \emph{overfitting} e uma
melhoria na regularização, porém, o treinamento da rede tende a demorar mais
gerando um \emph{tradeoff} entre \emph{overfitting} e tempo de
treinamento.\cite{srivastava2014dropout}

Está técnica não está restrita somente a redes \emph{feed forward} e pode ser
aplicadas a \emph{RBM} e \emph{autoencoders}, proporcionando uma maneira de
combinar exponencialmente diferentes arquiteturas
eficientemente.\cite{srivastava2014dropout}

\begin{figure}
  \caption{Uma rede neural com a aplicação de dropout}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsubsection{ReLU}

Em uma rede neural tradicional, as funções $\tanh(x)$ e $\frac{1}{1+e^{-x}}$
são usadas para introduzir não linearidade na computação do produto dos pesos
pelas entradas dos neurônios. Porém, estas funções possuem a desvantagem de
apresentarem o fenômeno de saturação. Isto ocorre quando a entrada da função
está distante da ponto central ($x=0$) e provoca uma perda de desempenho no
aprendizado.\cite{krizhevsky2012imagenet}

Por causa disto, o uso de \emph{rectifiers} tem se tornado cada vez mais comum
no treinamento de grandes redes. Este \emph{rectifier} é definido como:

$$ f(x) = \max(0,x) $$

Quando uma unidade ou neurônio usa este \emph{rectifier} ele é chamado de
Rectified Linear Unit ou ReLU.\@ De acordo com \citep{krizhevsky2012imagenet},
ReLUs são muito mais rápidos do que \emph{saturating non linearities} --- como
são conhecidas as funções mencionadas anteriormente --- e não requerem
regularização, embora isto ainda seja usado para aumentar o poder de
generalização da rede.

\begin{figure}
  \caption{Tipos de não linearidades aplicadas}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsubsection{Regularização}

Em uma típica rede \emph{deep}, existem milhares de pesos que sofrem adaptação
em seus valores. Normalmente um aproximador de função com este grande número de
parâmetros ajustáveis está sujeito a \emph{over fitting}. Isto reduz a
capacidade de generalização do modelo gerado para entradas que não fizeram
parte do treinamento. Redes neurais contornam este problema com o uso de
técnicas de regularização tais como:

\begin{itemize}

  \item \emph{Regularização L2}: Para cada peso $w$ da rede, é
    adicionado uma penalidade $\frac{1}{2}\lambda w^2$, onde $\lambda$ é a
    força de regularização. O termo $\frac{1}{2}$ é adicionado para facilitar o
    cálculo da derivada na fase de \emph{backpropagation} e o termo todo tem a
    interpretação de penalizar vetores de pesos com alguns poucos pesos muito
    altos e incentivar o uso de vetores difusos, ou seja, eles incentivam a
    rede a utilizar todos as suas entradas ao invés de usar demasiadamente uma
    entrada ou outra.

  \item \emph{Regularização L1}: Para cada peso $w$ da rede, é adicionado uma
    penalidade $\lambda|w|$ com a finalidade de tornar os pesos na rede
    esparsos (próximos de zero). Isto confere a rede um certo grau de
    invariabilidade à entradas com ruído. A regularização L1 pode ser usada
    juntamente com a L2, sendo chamadas neste caso de \emph{Elastic net
      regularization}.

  \item \emph{Max norm constraints}: Consiste em estipular um valor máximo para
    os pesos $w$ de maneira que a rede não se torne instável mesmo na presença
    de uma taxa de aprendizado muito grande.

  \item \emph{Dropout}: A técnica de \emph{dropout} também pode ser vista como
    uma regularização e pode ser usada juntamente com outros tipos.

\end{itemize}

% chktex-file 18 disable warnings about "
% chktex-file 19 disable warnings about ô
% chktex-file 6 disable italic correction warnings

\section{Deep Learning}

\subsection{Introdução}

A área de \emph{deep structured learning} que vem sendo conhecida como
\emph{deep learning} surgiu como um novo campo de pesquisa em aprendizado de
máquina nos últimos anos e as técnicas desenvolvidas nesta área já tem
impactado uma gama variada de trabalhos em processamento de sinais e
informações. Embora hajam diferentes definições, este trabalho adotará a
definição contida em \url{https://github.com/lisa-lab/DeepLearningTutorials},
visto que o mesmo trabalho foi utilizado como base para este\cite{deng2014deep}

\newtheorem{def-deep-learning}{Definição}

\begin{def-deep-learning}

  \begin{quote}

    \emph{Deep Learning} é uma nova área do aprendizado de máquina que vem
    sendo introduzida com o objetivo de aproximar o Aprendizado de Máquina do
    seu objetivo original: Inteligência Artificial. \emph{Deep Learning} aborda
    múltiplos níveis de representação e abstração que ajudam a processar dados
    tais como sons, imagens e texto.

  \end{quote}

\end{def-deep-learning}

A motivação para o uso de uma rede neural \emph{deep} ao invés das arquiteturas
tradicionais \emph{shallow}, tais como \emph{Support Vector Machines} visto que
esta última, a princípio consegue simular qualquer outra, é que existe um
\emph{tradeoff} entre espaço de memória e tempo de computação que pode ser
explorado para acelerar o processamento das redes. Uma maneira de entender este
\emph{tradeoff} em redes neurais é imaginar a saída das camadas intermediárias
como resultados intermediários da computação de uma rede.
\emph{shallow}\cite{bengio2007scaling}

Ainda assim,\emph{deep supervised learning} é uma tarefa difícil devido ao uso
de \emph{loss functions} não convexas presentes em problemas de grande
importância como computação a visual.

\subsection{Redes Convolucionais Modernas}

Redes convolucionais são um tipo especial de \emph{deep networks} projetadas
especificamente para lidar com dados vetoriais tais como imagens e sons.
Embora já fosse conhecida há algum tempo na área, este tipo de rede neural
ganhou fama na competição ILSVRC realizada em 2012 na qual ela diminuiu pela
metade o erro do melhor competidor. Este feito foi atingido com o uso de ReLUs
(\emph{rectified linear units}), GPUs e uma técnica conhecida como
\emph{dropout}. Desde então, Redes Convolucionais ganharam força e hoje é a
melhor técnica para reconhecimento de imagens.\cite{lecun2015deep}

Uma rede convolucional é baseada em 4 ideias principais: conexões locais, pesos
compartilhados, \emph{pooling} e múltiplas camadas. Conexões locais exploram o
fato de que em uma imagem existe uma correlação entre os valores dos pixels
vizinhos. Já pesos compartilhados geram o conceito de \emph{feature maps} e a
operação de \emph{pooling} (geralmente o máximo local) é usada pois é
invariante a posição o que da um maior poder de abstração para a
rede.\cite{lecun2015deep}

A operação de filtro é realizada pela função matemática de convolução, cujo
papel é detectar conjunções locais nas \emph{features} da camada anterior. Logo
em seguida vem a camada de \emph{pooling} cujo papel é combinar \emph{features}
semanticamente semelhantes.\cite{lecun2015deep}

A rede é tipicamente composta de dois a três estágios de convolução seguido da
aplicação de não linearidade e de \emph{pooling}. Por fim, são aplicadas mais
camadas convolucionais, desta vez completamente conectadas. O aprendizado é
feito através de \emph{backpropagation} como em redes neurais
tradicionais.\cite{lecun2015deep}

\subsubsection{Um breve histórico das CNNs}

A seguir, a listagem adaptada de~\cite{cs231n} das principais contribuições no
campo das redes convolucionais.

\begin{itemize}

  \item \emph{Neocognitron (1988)}: Criado por
    \citet{fukushima1988neocognitron}, foi o precursor das redes neurais
    convolucionais modernas. Consiste em múltiplas camadas de dois tipos de
    neurônios artificiais que tinham por objetivo simular a configuração
    observada por \citet{hubel1959receptive} em cérebros de gatos, os quais
    possuíam células responsáveis por tarefas distintas no reconhecimento
    visual.

  \item \emph{LeNet (1990)}: Proposta por \citet{le1990handwritten} foi usada
    com sucesso para reconhecimento de caracteres escritos a mão usando o
    dataset~MNIST.\@

  \item \emph{AlexNet (2012)}: \citet{krizhevsky2012imagenet} Foi a ganhadora
    da competição ILSVRC de 2012 e foi a responsável por grande parte do
    entusiasmo atual com redes convolucionais. Utiliza técnicas como
    \emph{droppout} e \emph{ReLUs}.

  \item \emph{ZF Net (2013)}: Se destacou na competição ILSVRC de 2013, por
    apresentar melhorias em relação à AlexNet, com a otimização de
    hyperparametros e a expansão do número de layers intermediários.
    \citet{zeiler2014visualizing}

  \item \emph{GoogLeNet (2014)}: A principal contribuição de
    \citet{szegedy2014going} foi o desenvolvimento de um \emph{Inception
      Module} que conseguiu reduzir o número de parâmetros (pesos) em 15 vezes.

  \item \emph{VGGNet (2014)}: \citet{zeiler2014visualizing} trouxe sua
    contribuição demonstrando que a profundidade da arquitetura é um componente
    crucial do desempenho da rede. Mais tarde foi descoberto que a sua
    performance supera a da GoogLeNet, embora necessite de mais memória e
    parâmetros. Hoje, é a rede preferida para extrair \emph{features} de
    imagens.

\end{itemize}

\subsubsection{Dropout}

O objetivo da técnica de \emph{dropout} é aproximar a solução ótima de calcular
a média dos resultados de treinamento de todas as combinações de parâmetros
possíveis. Isto é feito pela remoção aleatória e temporária de unidades da rede
bem como suas conexões. Cada unidade é mantida na rede com uma probabilidade
$p$, independente das outras.\cite{srivastava2014dropout}

Com a aplicação de \emph{dropout} há uma redução no \emph{overfitting} e uma
melhoria na regularização, porém, o treinamento da rede tende a demorar mais
gerando um \emph{tradeoff} entre \emph{overfitting} e tempo de
treinamento.\cite{srivastava2014dropout}

Está técnica não está restrita somente a redes \emph{feed forward} e pode ser
aplicadas a \emph{RBM} e \emph{autoencoders}, proporcionando uma maneira de
combinar exponencialmente diferentes arquiteturas
eficientemente.\cite{srivastava2014dropout}

\begin{figure}
  \caption{Uma rede neural com a aplicação de dropout}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}


\subsubsection{ReLU}

Em uma rede neural tradicional, as funções $\tanh(x)$ e $\frac{1}{1+e^{-x}}$ são
usadas para introduzir não linearidade na computação do produto dos pesos pelas
entradas dos neurônios. Porém, estas funções possuem a desvantagem de
apresentarem o fenômeno de saturação. Isto ocorre quando a entrada da função
está distante da ponto central ($x=0$) e provoca uma perda de desempenho no
aprendizado.\cite{krizhevsky2012imagenet}

Por causa disto, o uso de \emph{rectifiers} tem se tornado cada vez mais comum
no treinamento de grandes redes. Este \emph{rectifier} é definido como:

$$ f(x) = \max(0,x) $$

Quando uma unidade ou neurônio usa este \emph{rectifier} ele é chamado de
Rectified Linear Unit ou ReLU.\@ De acordo com \citep{krizhevsky2012imagenet},
ReLUs são muito mais rápidos do que \emph{saturating non linearities} --- como
são conhecidas as funções mencionadas anteriormente --- e não requerem
regularização, embora isto ainda seja usado para aumentar o poder de
generalização da rede.

\begin{figure}
  \caption{Tipos de não linearidades aplicadas}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsubsection{Regularização}

Em uma típica rede \emph{deep} existem milhares de pesos que sofrem adaptação
em seus valores. Normalmente um aproximador de função com este grande número de
parâmetros ajustáveis está sujeito a \emph{over fitting}. Redes neurais
contornam este problema com o uso de regularização tais como:

\begin{itemize}

  \item \emph{Regularização L2}: Para cada peso $w$ da rede, é
    adicionado uma penalidade $\frac{1}{2}\lambda w^2$, onde $\lambda$ é a
    força de regularização. O termo $\frac{1}{2}$ é adicionado para facilitar o
    cálculo da derivada na fase de \emph{backpropagation} e o termo todo tem a
    interpretação de penalizar vetores de pesos com alguns poucos pesos muito
    altos e incentivar o uso de vetores difusos, ou seja, eles incentivam a
    rede a utilizar todos as suas entradas ao invés de usar demasiadamente uma
    entrada ou outra.

  \item \emph{Regularização L1}: Para cada peso $w$ da rede, é adicionado uma
    penalidade $\lambda|w|$ com a finalidade de tornar os pesos na rede
    esparsos (próximos de zero). Isto confere a rede um certo grau de
    invariabilidade à entradas com ruído. A regularização L1 pode ser usada
    juntamente com a L2, sendo chamadas neste caso de \emph{Elastic net
      regularization}.

  \item \emph{Max norm constraints}: Consiste em estipular um valor máximo para
    os pesos $w$ de maneira que a rede não se torne instável mesmo na presença
    de uma taxa de aprendizado muito grande.

  \item \emph{Dropout}: A técnica de \emph{dropout} também pode ser vista como
    uma regularização e pode ser usada juntamente com outros tipos.

\end{itemize}

% chktex-file 18 disable warnings about "
% chktex-file 19 disable warnings about ô
% chktex-file 6 disable italic correction warnings


\chapter{Revisão sobre Redes Neurais}

As primeiras definições de uma rede neural artificial foram feitas em 1943 por
McCulloch e Pits inspiradas na hipótese de que as atividades mentais do cérebro
humano consistem em reações eletroquímicas em redes de células cerebrais
chamadas de neurônios. O neurônio artificial consiste em uma unidade
computacional abstrata capaz de receber estímulos de entrada e produzir uma ou
mais saídas e uma rede neural artificial --- de agora em diante referenciada
apenas por \emph{rede neural} --- consiste em um grafo direcionado cujos nodos
são neurônios artificiais.

\begin{figure}
  \caption{Abstração de um neurônio artificial}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsection{O neurônio artificial}

Cada neurônio recebe $m$ entradas ponderadas por um peso $w$,
correspondente a entrada do seu i-ésimo vizinho. Esta configuração foi
inspirada nos neurônios naturais cujas interconexões são reguladas por um
potencial de ativação responsável por diferenciar a intensidade entre as
mesmas. A saída é produzida aplicando-se a função limite $\sigma$ na soma
de todos as entradas ponderadas do neurônio. Esta função simula a \emph{lógica
  de limite} existente nos neurônios naturais, os quais somente irão propagar o
seu sinal caso haja um acúmulo suficiente de potencial elétrico no neurônio. Em
sua forma matemática, a saída $y$ do neurônio artificial pode ser expressada da
seguinte forma:

$$y=\sigma(\sum_{i=1}^{m}x_i w_i + b)$$

A função $\sigma$ representa na verdade uma classe de funções que podem ser
utilizadas dependendo do objetivo da construção da rede neural. Para problemas
de classificação \emph{linearmente separáveis}, pode ser usada a função limite
exemplificada na figura~\ref{fig:lim-functin}, enquanto para problemas não
linearmente separáveis é necessário utilizar algum tipo de função
\emph{sigmoide}. A função logística é um exemplo de função sigmoide
frequentemente utilizada por ser diferenciável em todo o seu domínio, o que
permite produzir garantias de convergência nos algoritmos de aprendizagem.

O valor $b$ corresponde ao \emph{bias} aplicado a função e pode ser omitido
simplesmente inserindo-se artificialmente uma nova entrada $x_0 = 1$.

\begin{figure}
\label{fig:lim-fun}
  \caption{Exemplo de funções de limite $\sigma$ usadas em neurônios artificiais}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsubsection{Bias}

Um neurônio artificial pode ser visto como um aproximador de funções. Mais
adiante<TODO, ref>, será explicado que tipo de função pode ser aproximada e em
qual situação, mas antes cabe aqui explicar o objetivo da entrada de
\emph{bias} ou \emph{viés}. Todo neurônio artificial é dito ter pelo menos uma
entrada extra $x_0 = 1$ e consequentemente, o peso extra associado $w_0$
chamado de bias $b$. O bias permite que a função aproximada pelo neurônio não
necessite obrigatoriamente passar pela origem $(0,0)$, o que reduz o erro médio
da aproximação caso os dados de treinamento não tenham sofrido nenhum tipo de
ajuste numérico como por exemplo a remoção do valor médio.

\begin{figure}
  \caption{Exemplo de função aproximada com e sem o vetor de bias}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

\subsubsection{Redes neurais}

Em uma rede neural, os neurônios são organizados em \emph{camadas}. Alguns
autores não consideram a camada de entrada como uma camada distinta, mas esta
abstração permite simplificar a notação e torna o processo de conectar redes
neurais entre si mais intuitivo. Outra camada especial é a de saída, pois é
ela que determina se a rede exercerá a função de um \emph{classificador} ou de
um \emph{regressor}. Todas as demais camadas intermediárias são chamadas de
\emph{camadas ocultas}; o nome não possui nenhuma conotação especial e serve
apenas para diferenciá-las das camadas de entrada e saída. Em uma rede
\emph{feed forward} as saídas de cada camada somente são computadas na direção
e sentido da camada de saída, isto é, não há uma retroalimentação das conexões
de saída para as de entrada. Quando isto acontece a rede é chamada de
\emph{recorrente} porém, o estudo deste tipo de rede neural não esta nos
objetivos deste trabalho.

Sendo assim, uma rede neural necessita ter pelo menos duas camadas --- a de
entrada e a de saída. A adição de camadas ou neurônios não possui um efeito
óbvio e generalizável para qualquer rede, e portanto é alvo de constante
experimentação e observação empírica sujeito a comportamento variável
dependendo do problema.

\begin{figure}\label{fig:003-nn}
  \caption{Exemplo de uma rede neural de 3 camadas}
  \begin{center}
    \includegraphics[scale=0.5]{placeholder}
  \end{center}
\end{figure}

Cada camada $S$ é formada por $R$ neurônios, cujos pesos das conexões podem ser
representados através de uma matriz $W$. A notação em forma vetorial e
matricial dos pesos das camadas permite que operações mais eficientes sejam
utilizadas para realizar a computação da saída da rede neural e do ajuste dos
mesmos.

$$ W_{S,R} =
\begin{bmatrix}
  w_{1,1} & w_{1,2} & \cdots & w_{1,R} \\
  w_{2,1} & w_{2,2} & \cdots & w_{2,R} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  w_{S,1} & w_{S,2} & \cdots & w_{S,R}
\end{bmatrix}
$$

A saída ou \emph{ativação} de uma camada $a'$ pode ser expressa em função da
ativação da camada anterior $a$ da seguinte maneira:

$$a^{'} = \sigma(Wa+b)$$

\subsubsection{Uma intuição sobre aprendizagem}

A aprendizagem em uma rede neural se da pelo ajuste dos pesos $w$ das conexões
dos neurônios. Ao longo dos anos foram propostas várias maneiras de atualizar
os pesos de uma rede neural automaticamente através de uma \emph{regra de
  aprendizagem}. Estas regras foram categorizadas em três grandes grupos:

\begin{description}
  \itemsep1pt\parskip0pt\parsep0pt

  \item[Aprendizagem Supervisionada] As entradas são acompanhadas por um
    conjunto de valores corretamente associados a mesma.

  \item[Aprendizagem Por Reforço] Semelhante ao aprendizado supervisionado,
    porém ao invés da resposta correta, é fornecido algo como uma nota ou uma
    indicação da performance da predição.

  \item[Aprendizagem Não Supervisionada] Nenhuma informação adicional a
    respeito das entradas é fornecida ao algoritmo.

\end{description}

Considere o problema de classificar corretamente os exemplos da
figura~\ref{fig:percetron-problem-layout}. Os pontos representados no plano dos
pesos das conexões são:

$$ \mathbf{x_0} =
\begin{bmatrix}
  1\\
  2
\end{bmatrix}
\mathbf{x_1} = \begin{bmatrix}
  3\\
  4
\end{bmatrix}
\mathbf{x_2} = \begin{bmatrix}
  5\\
  6
\end{bmatrix}
$$
\begin{figure}\label{fig:perceptron-problem-layout}
  \caption{Figura com o espaço de solução}
  \begin{center}
    \includegraphics[height=8cm]{placeholder}
  \end{center}
\end{figure}

O primeiro passo na formulação de um algoritmo de aprendizagem é a estipulação
de um valor inicial para os pesos das conexões dos neurônios, que pode ser
aleatória ou baseada em algum valor pré conhecido a respeito do problema e
depende das propriedades numéricas do mesmo. Para o exemplo do
\emph{perceptron} acima considere o vetor de pesos $w_0^T=[0.8,0.3]$.

\begin{figure}\label{fig:simple-perceptron}
  \caption{Uma rede neural simples de um único neurônio}
  \begin{center}
    \includegraphics[height=8cm]{placeholder}
  \end{center}
\end{figure}

É necessário também escolher uma \emph{função limite} $\sigma$. Para o exemplo
acima foi escolhida a função $hardlim$:
i
$$
hardlim(n) = \begin{cases}
  0 & \quad \text{if~} n<0\\
  1 & \quad \text{if~} n>=0\\
  \end{cases}
$$

\begin{figure}\label{fig:limit-functions-exapmple}
  \caption{Exemplo de funções de ativação}
  \begin{center}
    \includegraphics[height=8cm]{placeholder}
  \end{center}
  \legend{Na esquerda uma simples função de limite; à direita a função $\frac{1}{1+e^{-x}}$}
\end{figure}

% Quando a função de ativação $g$ representada na~\ref{fig:001-ann} for uma
% função limite[\ref{fig:002-limit-functions}], o neurônio artificial é chamado
% de \emph{perceptron}; Quando a função de ativação $g$ for uma função logística,
% o neurônio artificial é chamado de \emph{perceptron sigmoide} --- devido ao
% fato de que a função logística é um tipo particular de sigmoide. Estas funções
% são usadas para garantir que o neurônio consiga reproduzir funções não lineares
% sendo a principal vantagem da função sigmoide a sua diferenciabilidade em todo
% o seu domínio, fator que mais tarde será relevante na atualização dos pesos das
% conexões na rede.
